{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Mura.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N-ZCc3nab-o"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqcWNzahSC-K",
        "outputId": "60353f38-1421-49a1-bc6c-de8610c52f4e"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob7UajUwSFuh"
      },
      "source": [
        "import tensorflow_addons\n",
        "from tensorflow_addons.metrics import CohenKappa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82nFEA2TjZ3q"
      },
      "source": [
        "# Copy dataset to local storage from gDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afiSbqU-ajuW",
        "outputId": "8f132f2b-2bf0-4800-9e9a-d852c9c66e38"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwMPb1-0jSZc"
      },
      "source": [
        "!cp /content/drive/MyDrive/MURA/MURA-v1.1.zip .\n",
        "\n",
        "!unzip -q MURA-v1.1.zip\n",
        "\n",
        "!rm MURA-v1.1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N91hmuKab-s"
      },
      "source": [
        "# Create train and validation dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggqBTn15ab-v"
      },
      "source": [
        "root_path = Path('./MURA-v1.1')\n",
        "\n",
        "train_paths = pd.read_csv(root_path/'train_image_paths.csv', header=None, names=['filename'])\n",
        "valid_paths = pd.read_csv(root_path/'valid_image_paths.csv', header=None, names=['filename'])\n",
        "\n",
        "train_paths['class'] = (train_paths.filename.str.extract('(positive|negative)'))\n",
        "valid_paths['class'] = (valid_paths.filename.str.extract('(positive|negative)'))\n",
        "\n",
        "full_dataset = pd.concat([train_paths, valid_paths])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vG7IOJ6ab-x"
      },
      "source": [
        "# Create datagen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2ReI--IHKft"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=10,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1,\n",
        "                             zoom_range=0.3,\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest',\n",
        "                             rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlsdTAhbwIFb"
      },
      "source": [
        "# Function for training with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omanFpPoFwIY"
      },
      "source": [
        "def train_validate(model_base, dataset, n_folds, n_epochs):\n",
        "  kf = KFold(n_splits=n_folds, shuffle=True, random_state=27)\n",
        "  val_acc = []\n",
        "  val_loss = []\n",
        "\n",
        "  fold = 1\n",
        "  for train_index, val_index in kf.split(dataset):\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    train = dataset.iloc[train_index]\n",
        "    valid = dataset.iloc[val_index]\n",
        "    \n",
        "    train_datagen = datagen.flow_from_dataframe(train,\n",
        "                                                directory=root_path.parent,\n",
        "                                                target_size=(224, 224),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n",
        "\n",
        "    valid_datagen = datagen.flow_from_dataframe(valid,\n",
        "                                                directory=root_path.parent,\n",
        "                                                target_size=(224, 224),\n",
        "                                                batch_size=32,\n",
        "                                                class_mode='binary')\n",
        "    \n",
        "    model = tf.keras.models.clone_model(model_base)\n",
        "    model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'Training fold: {fold}')\n",
        "\n",
        "    hist = model.fit(train_datagen,epochs=n_epochs)\n",
        "    \n",
        "    scores = model.evaluate(valid_datagen, verbose=0)\n",
        "    print(f'Score for fold {fold}: {model.metrics_names[0]} of {scores[0]:.4f}; {model.metrics_names[1]} of {scores[1]:.4f}')\n",
        "    val_acc.append(scores[1])\n",
        "    val_loss.append(scores[0])\n",
        "\n",
        "    model.save(model.name + '_' + str(fold))\n",
        "    fold += 1\n",
        "  \n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Score per fold')\n",
        "  for i in range(0, len(val_acc)):\n",
        "    print('------------------------------------------------------------------------')\n",
        "    print(f'> Fold {i+1} - Loss: {val_loss[i]:.4f} - Accuracy: {val_acc[i]:.4f}')\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print('Average scores for all folds:')\n",
        "  print(f'> Accuracy: {np.mean(val_acc):.4f} (+- {np.std(val_acc):.4f})')\n",
        "  print(f'> Loss: {np.mean(val_loss):.4f}')\n",
        "  print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tb8FLeab-0"
      },
      "source": [
        "#DenseNet with custom layers\n",
        "\n",
        "Using DenseNet169 model with pretrained weights from ImageNet, top layer is replaced with a number of custom layers which are trained, DenseNet169 layers are frozen and then fine-tuned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l368FD-rML9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05779efd-5056-4085-f7d8-765b8ffc22af"
      },
      "source": [
        "# Download DenseNet169\n",
        "densenet = tf.keras.applications.DenseNet169(weights='imagenet',\n",
        "                                             include_top = False,\n",
        "                                             input_shape=(224, 224, 3))\n",
        "densenet.trainable = False\n",
        "\n",
        "# Build the model with DenseNet and custom layers on top\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = densenet(inputs, training=False)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(units=256,activation='relu')(x)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "\n",
        "x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "x = tf.keras.layers.Dense(units=128,activation='relu')(x)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name='densenet169_custom')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "51879936/51877672 [==============================] - 2s 0us/step\n",
            "Model: \"densenet169_custom\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "densenet169 (Functional)     (None, 7, 7, 1664)        12642880  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 81536)             0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 81536)             326144    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               20873472  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 33,876,545\n",
            "Trainable params: 21,070,081\n",
            "Non-trainable params: 12,806,464\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuupOkKVb6V2"
      },
      "source": [
        "# Trying to train longer on normal train validation split\n",
        "\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "                            optimizer=tf.keras.optimizers.Adam(),\n",
        "                            metrics=[cohen_kappa_score])\n",
        "\n",
        "hist = model.fit(train_gen, epochs=30, validation_data=valid_gen)\n",
        "# model.save('densenetCustom')\n",
        "# !cp -r ./densenetCustom /content/drive/MyDrive/MURA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stAYYZ21ab-3"
      },
      "source": [
        "###Train with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ugxQXIab-4",
        "outputId": "d662d938-a93b-4c9c-dc62-9f433a029b27"
      },
      "source": [
        "train_validate(model, full_dataset, 5, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 32004 validated image filenames belonging to 2 classes.\n",
            "Found 8001 validated image filenames belonging to 2 classes.\n",
            "------------------------------------------------------------------------\n",
            "Training fold: 1\n",
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 436s 428ms/step - loss: 0.7341 - accuracy: 0.5514\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 430s 430ms/step - loss: 0.6762 - accuracy: 0.5823\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6697 - accuracy: 0.5886\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6684 - accuracy: 0.5926\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6694 - accuracy: 0.5847\n",
            "Score for fold 1: loss of 0.6614; accuracy of 0.5987\n",
            "INFO:tensorflow:Assets written to: densenet169_1/assets\n",
            "Found 32004 validated image filenames belonging to 2 classes.\n",
            "Found 8001 validated image filenames belonging to 2 classes.\n",
            "------------------------------------------------------------------------\n",
            "Training fold: 2\n",
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 435s 427ms/step - loss: 0.7360 - accuracy: 0.5540\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6720 - accuracy: 0.5842\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6653 - accuracy: 0.5911\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 426s 426ms/step - loss: 0.6662 - accuracy: 0.5939\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6665 - accuracy: 0.5913\n",
            "Score for fold 2: loss of 0.6585; accuracy of 0.6082\n",
            "INFO:tensorflow:Assets written to: densenet169_2/assets\n",
            "Found 32004 validated image filenames belonging to 2 classes.\n",
            "Found 8001 validated image filenames belonging to 2 classes.\n",
            "------------------------------------------------------------------------\n",
            "Training fold: 3\n",
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 440s 429ms/step - loss: 0.7499 - accuracy: 0.5429\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6727 - accuracy: 0.5909\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6665 - accuracy: 0.5948\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6633 - accuracy: 0.5965\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 428s 428ms/step - loss: 0.6652 - accuracy: 0.5899\n",
            "Score for fold 3: loss of 0.6563; accuracy of 0.6143\n",
            "INFO:tensorflow:Assets written to: densenet169_3/assets\n",
            "Found 32004 validated image filenames belonging to 2 classes.\n",
            "Found 8001 validated image filenames belonging to 2 classes.\n",
            "------------------------------------------------------------------------\n",
            "Training fold: 4\n",
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 438s 428ms/step - loss: 0.7315 - accuracy: 0.5611\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6699 - accuracy: 0.5914\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 429s 428ms/step - loss: 0.6654 - accuracy: 0.5971\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6614 - accuracy: 0.6027\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6609 - accuracy: 0.5984\n",
            "Score for fold 4: loss of 0.6600; accuracy of 0.6043\n",
            "INFO:tensorflow:Assets written to: densenet169_4/assets\n",
            "Found 32004 validated image filenames belonging to 2 classes.\n",
            "Found 8001 validated image filenames belonging to 2 classes.\n",
            "------------------------------------------------------------------------\n",
            "Training fold: 5\n",
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 434s 426ms/step - loss: 0.7324 - accuracy: 0.5585\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 427s 427ms/step - loss: 0.6752 - accuracy: 0.5780\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 428s 427ms/step - loss: 0.6658 - accuracy: 0.5929\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 431s 430ms/step - loss: 0.6645 - accuracy: 0.5976\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 440s 439ms/step - loss: 0.6605 - accuracy: 0.6012\n",
            "Score for fold 5: loss of 0.6558; accuracy of 0.6169\n",
            "INFO:tensorflow:Assets written to: densenet169_5/assets\n",
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.6614 - Accuracy: 0.5987\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.6585 - Accuracy: 0.6082\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.6563 - Accuracy: 0.6143\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.6600 - Accuracy: 0.6043\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 0.6558 - Accuracy: 0.6169\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 0.6085 (+- 0.0066157083894466924:.4f)\n",
            "> Loss: 0.6584\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I55OELwPXGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c3c78e2-710f-4fb3-cfaa-7f24e216d557"
      },
      "source": [
        "# Storing model on gDrive for later use\n",
        "!cp -r ./densenet169_5 /content/drive/MyDrive/MURA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './densenet169_5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k7TogKBJlmu"
      },
      "source": [
        "### Fine tuning on one of the folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrJXZsxYndGm"
      },
      "source": [
        "reconstructed_model = tf.keras.models.load_model(model.name + '_5')\n",
        "\n",
        "# Freeze densenet layers\n",
        "densenet_layer = reconstructed_model.layers[1]\n",
        "for layer in densenet_layer.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "# Unfreezing top 30 layers\n",
        "for layer in densenet_layer.layers[-30:]:\n",
        "  if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "    layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU8WR9trOFLD",
        "outputId": "3c27a2e7-98ea-430a-b38c-55df01f07d5b"
      },
      "source": [
        "# Verifying that layers are not frozen\n",
        "dl = reconstructed_model.layers[1]\n",
        "for layer in dl.layers[-30:]:\n",
        "  print(layer.name + \" \" + str(layer.trainable))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv5_block29_0_bn False\n",
            "conv5_block29_0_relu True\n",
            "conv5_block29_1_conv True\n",
            "conv5_block29_1_bn False\n",
            "conv5_block29_1_relu True\n",
            "conv5_block29_2_conv True\n",
            "conv5_block29_concat True\n",
            "conv5_block30_0_bn False\n",
            "conv5_block30_0_relu True\n",
            "conv5_block30_1_conv True\n",
            "conv5_block30_1_bn False\n",
            "conv5_block30_1_relu True\n",
            "conv5_block30_2_conv True\n",
            "conv5_block30_concat True\n",
            "conv5_block31_0_bn False\n",
            "conv5_block31_0_relu True\n",
            "conv5_block31_1_conv True\n",
            "conv5_block31_1_bn False\n",
            "conv5_block31_1_relu True\n",
            "conv5_block31_2_conv True\n",
            "conv5_block31_concat True\n",
            "conv5_block32_0_bn False\n",
            "conv5_block32_0_relu True\n",
            "conv5_block32_1_conv True\n",
            "conv5_block32_1_bn False\n",
            "conv5_block32_1_relu True\n",
            "conv5_block32_2_conv True\n",
            "conv5_block32_concat True\n",
            "bn False\n",
            "relu True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvjKgdJcThbM"
      },
      "source": [
        "### Split data according to corresponding fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ER3b_qRpfm"
      },
      "source": [
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=27)\n",
        "folds = []\n",
        "for ftrain, fvalid in  kf.split(full_dataset):\n",
        "  folds.append((ftrain, fvalid))\n",
        "\n",
        "ftrain = full_dataset.iloc[folds[4][0]]\n",
        "fvalid = full_dataset.iloc[folds[4][1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3fLDiT0UOz1"
      },
      "source": [
        "### Create generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DqBuHhQTxgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "356f85e6-bb8d-457e-ebdf-460900f928cd"
      },
      "source": [
        "ftrain_datagen = datagen.flow_from_dataframe(ftrain,\n",
        "                                            directory=root_path.parent,\n",
        "                                            target_size=(224, 224),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')\n",
        "\n",
        "fvalid_datagen = datagen.flow_from_dataframe(fvalid,\n",
        "                                            directory=root_path.parent,\n",
        "                                            target_size=(224, 224),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b711a4e75739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ftrain_datagen = datagen.flow_from_dataframe(ftrain,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                             \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                             \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             class_mode='binary')\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ftrain' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK7H1Wp-UYBW"
      },
      "source": [
        "### Fine tune model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySJvCZpUUa9k",
        "outputId": "69807989-a6ce-40d2-c459-09e41c385274"
      },
      "source": [
        "reconstructed_model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "                            optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "hist = reconstructed_model.fit(ftrain_datagen, epochs=5)\n",
        "reconstructed_model.evaluate(fvalid_datagen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1001/1001 [==============================] - 470s 462ms/step - loss: 0.6615 - accuracy: 0.6023\n",
            "Epoch 2/5\n",
            "1001/1001 [==============================] - 455s 455ms/step - loss: 0.6592 - accuracy: 0.6029\n",
            "Epoch 3/5\n",
            "1001/1001 [==============================] - 456s 455ms/step - loss: 0.6505 - accuracy: 0.6119\n",
            "Epoch 4/5\n",
            "1001/1001 [==============================] - 453s 452ms/step - loss: 0.6548 - accuracy: 0.6098\n",
            "Epoch 5/5\n",
            "1001/1001 [==============================] - 454s 453ms/step - loss: 0.6491 - accuracy: 0.6122\n",
            "251/251 [==============================] - 121s 471ms/step - loss: 0.6545 - accuracy: 0.6034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6544771790504456, 0.6034245491027832]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD7v7TwS-e_U"
      },
      "source": [
        "# Model comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCgBQy-Ql9lA"
      },
      "source": [
        "## Create generators used without cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-MjC1-vl9DP",
        "outputId": "076e82d8-261a-45e1-a2c4-ddff00a9c867"
      },
      "source": [
        "train_gen = datagen.flow_from_dataframe(train_paths,\n",
        "                                        directory=root_path.parent,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=32,\n",
        "                                        class_mode='binary')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "valid_gen = valid_datagen.flow_from_dataframe(valid_paths,\n",
        "                                        directory=root_path.parent,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=32,\n",
        "                                        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36808 validated image filenames belonging to 2 classes.\n",
            "Found 3197 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtadXgnwzgZg"
      },
      "source": [
        "# DenseNet169"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6eokONPzfi-",
        "outputId": "af08421a-3d10-4dfd-a117-48fb4e5167cd"
      },
      "source": [
        "# Download DenseNet169\n",
        "densenet169 = tf.keras.applications.DenseNet169(weights='imagenet',\n",
        "                                       include_top = False,\n",
        "                                       input_shape=(224, 224, 3))\n",
        "densenet169.trainable = False\n",
        "\n",
        "# Build the model with DenseNet169 and custom layers on top\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = densenet169(inputs, training=False)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name='densenet169')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"densenet169\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "densenet169 (Functional)     (None, 7, 7, 1664)        12642880  \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 81536)             0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 81536)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 81537     \n",
            "=================================================================\n",
            "Total params: 12,724,417\n",
            "Trainable params: 81,537\n",
            "Non-trainable params: 12,642,880\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcZNPnFczseJ",
        "outputId": "b9a86cd7-cc47-4ce3-9cf7-6fe47301e6fc"
      },
      "source": [
        "kappa_metric = CohenKappa(num_classes=2, name='cohenkappa')\n",
        "\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "                            optimizer=tf.keras.optimizers.Adam(),\n",
        "                            metrics=kappa_metric)\n",
        "\n",
        "checkpoint_filepath = 'densenet.{epoch:02d}-{val_loss:.2f}'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_cohenkappa',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "hist = model.fit(train_gen, epochs=5, validation_data=valid_gen, callbacks=[model_checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1151/1151 [==============================] - 678s 580ms/step - loss: 1.6004 - cohenkappa: 0.2989 - val_loss: 1.4988 - val_cohenkappa: 0.3616\n",
            "Epoch 2/5\n",
            "1151/1151 [==============================] - 664s 576ms/step - loss: 1.6414 - cohenkappa: 0.3079 - val_loss: 1.9616 - val_cohenkappa: 0.3208\n",
            "Epoch 3/5\n",
            "1151/1151 [==============================] - 664s 577ms/step - loss: 1.6727 - cohenkappa: 0.3184 - val_loss: 2.3189 - val_cohenkappa: 0.3194\n",
            "Epoch 4/5\n",
            "1151/1151 [==============================] - 666s 579ms/step - loss: 1.7406 - cohenkappa: 0.3125 - val_loss: 1.4524 - val_cohenkappa: 0.3649\n",
            "Epoch 5/5\n",
            "1151/1151 [==============================] - 665s 578ms/step - loss: 1.8446 - cohenkappa: 0.3056 - val_loss: 1.8445 - val_cohenkappa: 0.3685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPIgnu4gAtZY",
        "outputId": "145ef006-12cf-46c7-f2f6-d39ff258d776"
      },
      "source": [
        "\n",
        "model.save(model.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _cast_ypred, _update_multi_class_model while saving (showing 2 of 2). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as _cast_ypred, _update_multi_class_model while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: densenet169/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: densenet169/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y2WDSb_Y6iP"
      },
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfXgbqONY5jO",
        "outputId": "1a879fa2-a7b0-4788-83fc-c92ebc398b06"
      },
      "source": [
        "# Download VGG16\n",
        "vgg16 = tf.keras.applications.VGG16(weights='imagenet',\n",
        "                                       include_top = False,\n",
        "                                       input_shape=(224, 224, 3))\n",
        "vgg16.trainable = False\n",
        "\n",
        "# Build the model with VGG16 and custom layers on top\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = vgg16(inputs, training=False)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name='vgg16')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25089     \n",
            "=================================================================\n",
            "Total params: 14,739,777\n",
            "Trainable params: 25,089\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV3J1icSm0LP",
        "outputId": "d555080b-ff3d-41e9-a947-5b0f6b01dcbb"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "                            optimizer=tf.keras.optimizers.Adam(),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(train_gen, epochs=5, validation_data=valid_gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1151/1151 [==============================] - 549s 475ms/step - loss: 0.6737 - accuracy: 0.6200 - val_loss: 0.6181 - val_accuracy: 0.6606\n",
            "Epoch 2/5\n",
            "1151/1151 [==============================] - 547s 475ms/step - loss: 0.6300 - accuracy: 0.6639 - val_loss: 0.6080 - val_accuracy: 0.6728\n",
            "Epoch 3/5\n",
            "1151/1151 [==============================] - 547s 475ms/step - loss: 0.6368 - accuracy: 0.6545 - val_loss: 0.6058 - val_accuracy: 0.6600\n",
            "Epoch 4/5\n",
            "1151/1151 [==============================] - 546s 474ms/step - loss: 0.6345 - accuracy: 0.6642 - val_loss: 0.6447 - val_accuracy: 0.6362\n",
            "Epoch 5/5\n",
            "1151/1151 [==============================] - 544s 472ms/step - loss: 0.6404 - accuracy: 0.6646 - val_loss: 0.6057 - val_accuracy: 0.6697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7FIbAHzFI6",
        "outputId": "51bc9af1-5cd9-4a20-be38-3f622ab0feb1"
      },
      "source": [
        "model.save(model.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: vgg16/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM4b8e-5Znm0"
      },
      "source": [
        "# ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQiT75bwZvJO",
        "outputId": "9c2deb34-6dd6-4faa-85d8-fe85f86bde9e"
      },
      "source": [
        "# Download ResNet50\n",
        "resnet50 = tf.keras.applications.ResNet50(weights='imagenet',\n",
        "                                          include_top = False,\n",
        "                                          input_shape=(224, 224, 3))\n",
        "resnet50.trainable = False\n",
        "\n",
        "# Build the model with ResNet50 and custom layers on top\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "\n",
        "x = resnet50(inputs, training=False)\n",
        "\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs, outputs, name='resnet50')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "Model: \"resnet50\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100352)            0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 100353    \n",
            "=================================================================\n",
            "Total params: 23,688,065\n",
            "Trainable params: 100,353\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xbo69NXzOgS",
        "outputId": "c47cc503-9297-49fe-eab5-d1c2af54ceaa"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.binary_crossentropy,\n",
        "                            optimizer=tf.keras.optimizers.Adam(),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(train_gen, epochs=5, validation_data=valid_gen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1151/1151 [==============================] - 550s 475ms/step - loss: 0.8956 - accuracy: 0.5484 - val_loss: 0.7662 - val_accuracy: 0.5161\n",
            "Epoch 2/5\n",
            "1151/1151 [==============================] - 545s 473ms/step - loss: 0.7962 - accuracy: 0.5652 - val_loss: 0.6619 - val_accuracy: 0.5887\n",
            "Epoch 3/5\n",
            "1151/1151 [==============================] - 546s 474ms/step - loss: 0.8330 - accuracy: 0.5569 - val_loss: 0.6635 - val_accuracy: 0.6056\n",
            "Epoch 4/5\n",
            "1151/1151 [==============================] - 543s 472ms/step - loss: 0.7998 - accuracy: 0.5610 - val_loss: 0.8188 - val_accuracy: 0.5252\n",
            "Epoch 5/5\n",
            "1151/1151 [==============================] - 543s 472ms/step - loss: 0.8165 - accuracy: 0.5591 - val_loss: 0.6993 - val_accuracy: 0.5840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9oOa5CCz2aE",
        "outputId": "e0fe0b79-34f6-4b07-c789-72a20832ce5b"
      },
      "source": [
        "model.save(model.name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: resnet50/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXhtzs1S0Jr_"
      },
      "source": [
        "# Store trained models on gDrive for later use\n",
        "!cp -r ./vgg16 /content/drive/MyDrive/MURA\n",
        "!cp -r ./resnet50 /content/drive/MyDrive/MURA\n",
        "!cp -r ./densenet169 /content/drive/MyDrive/MURA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "RtPgg3bbab-6",
        "outputId": "7a9a7025-a4dc-41d0-c232-68c1ed149f5d"
      },
      "source": [
        "plt.plot(hist.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='right')\n",
        "plt.xticks(np.arange(3))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUPMGFlgdSSB"
      },
      "source": [
        "# Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYGCg6sXJoTP"
      },
      "source": [
        "from IPython.display import Image\n",
        "import matplotlib.cm as cm\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUQg2EQw4AAR",
        "outputId": "250a9cde-88b7-4a20-df5c-2f730b3a8d3f"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/MURA/vgg16 ./\n",
        "model = tf.keras.models.load_model('vgg16')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 25089     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,739,777\n",
            "Trainable params: 25,089\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_16 = model.get_layer('vgg16')\n",
        "vgg_16.layers\n",
        "\n",
        "\n",
        "grad_model = keras.Model(vgg_16.inputs, vgg_16.output)\n",
        "grad_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq2vQKKlNJHB",
        "outputId": "1692f1ac-932f-4d48-e459-fce32ebce3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eCONUUkI_xP"
      },
      "source": [
        "\n",
        "prediction = np.round(np.asarray(model.predict(valid_gen)))\n",
        "prediction = prediction.astype(int).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS-bcgtPLBpR",
        "outputId": "322cd19c-9e11-4dbd-8fb9-2407fc057ca3"
      },
      "source": [
        "metric = CohenKappa(num_classes=2)\n",
        "metric.update_state(valid_gen.classes, prediction)\n",
        "result = metric.result()\n",
        "print(\"Cohen Kappa score\", result.numpy())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen Kappa score -0.026060343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPDXMFASFfRU"
      },
      "source": [
        "img_size = (224, 224)\n",
        "\n",
        "last_conv_layer_name = \"conv5_block32_concat\"\n",
        "classifier_layer_names = [\n",
        "    \"flatten_1\",\n",
        "    \"dropout_1\",\n",
        "    \"dense_1\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZiY3PBgJxaA"
      },
      "source": [
        "def get_img_array(img_path, size):\n",
        "    # `img` is a PIL image of size 299x299\n",
        "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
        "    array = keras.preprocessing.image.img_to_array(img)\n",
        "    # We add a dimension to transform our array into a \"batch\"\n",
        "    # of size (1, 299, 299, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "\n",
        "def make_gradcam_heatmap(\n",
        "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
        "):\n",
        "    # First, we create a model that maps the input image to the activations\n",
        "    # of the last conv layer\n",
        "    last_conv_layer = model.get_layer('densenet169').get_layer(last_conv_layer_name)\n",
        "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "    # Second, we create a model that maps the activations of the last conv\n",
        "    # layer to the final class predictions\n",
        "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "    for layer_name in classifier_layer_names:\n",
        "        x = model.get_layer(layer_name)(x)\n",
        "    classifier_model = keras.Model(classifier_input, x)\n",
        "\n",
        "    # Then, we compute the gradient of the top predicted class for our input image\n",
        "    # with respect to the activations of the last conv layer\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute activations of the last conv layer and make the tape watch it\n",
        "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "        # Compute class predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "        top_pred_index = tf.argmax(preds[0])\n",
        "        top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "    # This is the gradient of the top predicted class with regard to\n",
        "    # the output feature map of the last conv layer\n",
        "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
        "\n",
        "    # This is a vector where each entry is the mean intensity of the gradient\n",
        "    # over a specific feature map channel\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # We multiply each channel in the feature map array\n",
        "    # by \"how important this channel is\" with regard to the top predicted class\n",
        "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "    pooled_grads = pooled_grads.numpy()\n",
        "    for i in range(pooled_grads.shape[-1]):\n",
        "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "\n",
        "    # The channel-wise mean of the resulting feature map\n",
        "    # is our heatmap of class activation\n",
        "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
        "\n",
        "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDbXztwXFIsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "7623c3ed-4ae4-4e33-b29e-3a68d1aaf36c"
      },
      "source": [
        "# Prepare image\n",
        "img_array = get_img_array(full_dataset.iloc[15][0], (224, 224))\n",
        "\n",
        "# Print what the top predicted class is\n",
        "preds = model.predict(img_array)\n",
        "print(\"Predicted:\", preds[0][0])\n",
        "\n",
        "# Generate class activation heatmap\n",
        "heatmap = make_gradcam_heatmap(\n",
        "    img_array,\n",
        "    model,\n",
        "    last_conv_layer_name,\n",
        "    classifier_layer_names\n",
        ")\n",
        "\n",
        "# Display heatmap\n",
        "plt.matshow(heatmap)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8c9c43f126c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlast_conv_layer_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mclassifier_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-9060999c7016>\u001b[0m in \u001b[0;36mmake_gradcam_heatmap\u001b[0;34m(img_array, model, last_conv_layer_name, classifier_layer_names)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# of the last conv layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlast_conv_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'densenet169'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_conv_layer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlast_conv_layer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_conv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Second, we create a model that maps the activations of the last conv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                   for t in tf.nest.flatten(inputs)]):\n\u001b[1;32m    145\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctional_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_graph_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 230\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;34mf'Graph disconnected: cannot obtain value for tensor {x} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m                 \u001b[0;34mf'at layer \"{layer.name}\". The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 f'were accessed without issue: {layers_with_complete_input}')\n",
            "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") at layer \"zero_padding2d\". The following previous layers were accessed without issue: []"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7qo6h9hWkw6"
      },
      "source": [
        "# We load the original image\n",
        "img = keras.preprocessing.image.load_img(full_dataset.iloc[15][0])\n",
        "img = keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "# We rescale heatmap to a range 0-255\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# We use jet colormap to colorize heatmap\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "# We use RGB values of the colormap\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "# We create an image with RGB colorized heatmap\n",
        "jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "# Superimpose the heatmap on original image\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "# Save the superimposed image\n",
        "save_path = \"xray_cam.jpg\"\n",
        "superimposed_img.save(save_path)\n",
        "\n",
        "# Display Grad CAM\n",
        "display(Image(save_path))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ve3K2bcOICOE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}