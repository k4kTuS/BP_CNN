{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMkTY-iaHz5m"
   },
   "source": [
    "# Training notebook\n",
    "\n",
    "* This notebook is used for building, training and evaluating CNN models on MURA dataset.  \n",
    "* For the purpose of experimentation, the CNN model, dataset type (original / preprocessed offline), online augmentation and preprocessing can be easily changed with the use of custom loading functions\n",
    "* More information on used functions can be found mainly in python files ***utils.py*** and ***image_preprocessing.py***\n",
    "* Some cells are specific for Google Colab environment and might not work correctly in other environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_N-ZCc3nab-o",
    "outputId": "1ca95909-341d-42ad-aeb3-4ae646456ef6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10 kB 30.4 MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20 kB 22.5 MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30 kB 17.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40 kB 15.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51 kB 7.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 61 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71 kB 9.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81 kB 9.3 MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 92 kB 10.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 112 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 122 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 133 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 143 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 153 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 163 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 174 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 184 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 194 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 204 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 215 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 225 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 235 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 245 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 256 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 266 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 276 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 286 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 296 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 307 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 317 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 327 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 337 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 348 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 358 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 368 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 378 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 389 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 399 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 409 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 419 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 430 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 440 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 450 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 460 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 471 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 481 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 491 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 501 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 512 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 522 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 532 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 542 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 552 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 563 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 573 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 583 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 593 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 604 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 614 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 624 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 634 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 645 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 655 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 665 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 675 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 686 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 696 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 706 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 716 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 727 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 737 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 747 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 757 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 768 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 778 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 788 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 798 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 808 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 819 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 829 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 839 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 849 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 860 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 870 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 880 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 890 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 901 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 911 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 921 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 931 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 942 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 952 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 962 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 972 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 983 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 993 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.0 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.0 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.0 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.0 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.0 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.1 MB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1 MB 8.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.16.1\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Google Colab does not have tensorflow_addons installed by default\n",
    "!pip install tensorflow-addons\n",
    "from tensorflow_addons.metrics import CohenKappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A3VbOUEYeUG"
   },
   "source": [
    "# FILEPATHS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "chR2OBqAYd4V"
   },
   "outputs": [],
   "source": [
    "DRIVE_DIR = '/content/drive/MyDrive/MURA/' # Directory in my personal Google Drive with all source files and datasets\n",
    "MURA_DIR = '/content/original/' # Directory with original MURA dataset\n",
    "CLAHE_2_DIR = '/content/clahe_2/' # Directory for CLAHE preprocessed dataset with clipLimit=2\n",
    "CLAHE_10_DIR = '/content/clahe_10/' # Directory for CLAHE preprocessed dataset with clipLimit=10\n",
    "CROPPED_DIR = '/content/cropped/' # Directory for custom cropping preprocessed dataset\n",
    "DATAFRAME_PATH = DRIVE_DIR + '/tvt_detailed_paths.csv' # Path to csv file with dataset information (train-valid-test split)\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/MURA/models/' # Root directory for storing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djVz8HVw7oEY"
   },
   "source": [
    "# Google Colab specific section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82nFEA2TjZ3q"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afiSbqU-ajuW",
    "outputId": "ee6dd56f-f55b-4226-b8aa-0cfa89380f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMOttX7zz_tP"
   },
   "source": [
    "## Google colab functions\n",
    "\n",
    "Next cell containts function definitions, that are needed or useful when running notebook in Google Colab, such as copying dataset from Google Drive, checking GPU utilization, copying other files from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JdCdOOo5z5kU"
   },
   "outputs": [],
   "source": [
    "# This function is from Google Colab guide notebooks: https://colab.research.google.com/notebooks/pro.ipynb\n",
    "def check_gpu():\n",
    "  \"\"\"\n",
    "  Prints information about available GPU or message saying there isn't one\n",
    "  \"\"\"\n",
    "  gpu_info = !nvidia-smi\n",
    "  gpu_info = '\\n'.join(gpu_info)\n",
    "  if gpu_info.find('failed') >= 0:\n",
    "    print('Not connected to a GPU')\n",
    "  else:\n",
    "    print(gpu_info)\n",
    "\n",
    "\n",
    "def init_file_struct():\n",
    "  \"\"\"\n",
    "  Prep file structure in Google Colab and copies neccessary files from Google Drive\n",
    "  \"\"\"\n",
    "  !cp \"{DRIVE_DIR}src/utils.py\" .\n",
    "  !cp \"{DRIVE_DIR}src/image_preprocessing.py\" .\n",
    "\n",
    "\n",
    "def copy_dataset(filename):\n",
    "  \"\"\"\n",
    "  Copies and unzips dataset from Google Drive and deletes the zipped file\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  filename : str\n",
    "    Dataset filename, without its path, the path will be added from constants\n",
    "  \"\"\"\n",
    "  !cp {DRIVE_DIR}{filename} /content/\n",
    "\n",
    "  !unzip -q /content/{filename}\n",
    "\n",
    "  !rm /content/{filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iQ5LRSN5bKU"
   },
   "source": [
    "## Import python files after copying them from Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lA5yYz1FsD4f"
   },
   "outputs": [],
   "source": [
    "init_file_struct()\n",
    "from utils import *\n",
    "from image_preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foQIO96o5k_g"
   },
   "source": [
    "## Copy desired dataset to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "T02MSGga5kNU"
   },
   "outputs": [],
   "source": [
    "# # copy_dataset('original.zip')\n",
    "# # copy_dataset('clahe_2.zip')\n",
    "# copy_dataset('clahe_10.zip')\n",
    "copy_dataset('cropped.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OX41Ziahh2F6"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQMhRoXwelSv"
   },
   "outputs": [],
   "source": [
    "# Functions used to preprocess images according to ImageNet \n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "PgUXgWiYh9Gl"
   },
   "outputs": [],
   "source": [
    "# DATAFRAME\n",
    "BODY_PART = 'ALL' # Which body part to use, can be \"ALL\" for full dataset\n",
    "# IMAGE AUGMENTATION\n",
    "ROTATION_R = 20 # Rotation range\n",
    "W_SHIFT_R = 0.05 # Width shift range\n",
    "H_SHIFT_R = 0.05 # Height shift range\n",
    "BRIGHTNESS_R = (0.9, 1.1) # Brightness range\n",
    "ZOOM_R = 0.1 # Zoom range\n",
    "H_FLIP = True # Flip the image horizontally\n",
    "PRE_FUNC = rescale # Preprocessing function to be applied to every image before feeding it to the model\n",
    "# DATAFRAME FLOW\n",
    "FLOW_DIR = CLAHE_10_DIR # Directory containing dataset images, choose one from filepath constants or use your own\n",
    "BATCH_SIZE = 32 # Number of images per batch\n",
    "IMAGE_SIZE = (224, 224) # Resize all images to this size\n",
    "# MODEL\n",
    "BASE_NAME = 'ResNet50' # Name corresponding to one of the architectures from Keras functional API\n",
    "WEIGHTS = 'imagenet' # ImageNet pre-trained weights or a path to stored model weights\n",
    "INPUT_SHAPE = (224, 224, 3) # Model input shape\n",
    "POOLING = 'avg' # Pooling layer used after the last convolutional layer, can be None\n",
    "OPTIMIZER = Adam(learning_rate=0.0001) # Optimzer used during training\n",
    "MODEL_NAME = BASE_NAME + '_tmp' # Name used for storing model weights during and after training\n",
    "# TRAINING\n",
    "EPOCHS = 10 # Number of epochs for trainig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vG7IOJ6ab-x"
   },
   "source": [
    "# Create datagens and flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggqBTn15ab-v",
    "outputId": "59da9a69-54e6-40a3-e72d-327e54391937"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36260 validated image filenames belonging to 2 classes.\n",
      "Found 3197 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create dataframes from train valid split\n",
    "train_df = get_dataframe(body_part=BODY_PART, split='train', path=DATAFRAME_PATH)\n",
    "valid_df = get_dataframe(body_part=BODY_PART, split='valid', path=DATAFRAME_PATH)\n",
    "\n",
    "# Create ImageDataGenerators, train_gen uses specified online augmentation, valid_gen only preprocesses images\n",
    "train_gen, valid_gen = create_generators(rotation_r=ROTATION_R,\n",
    "                                         w_shift_r=W_SHIFT_R,\n",
    "                                         h_shift_r=H_SHIFT_R,\n",
    "                                         brightness_r=BRIGHTNESS_R,\n",
    "                                         zoom_r=ZOOM_R,\n",
    "                                         h_flip=H_FLIP,\n",
    "                                         pre_func=PRE_FUNC)\n",
    "# Create dataframe flows, filepaths in dataframes are not absolute, so set directory parameter correctly\n",
    "train_flow, valid_flow = create_dataframe_flows(train_gen=train_gen,\n",
    "                                                valid_gen=valid_gen,\n",
    "                                                train_df=train_df,\n",
    "                                                valid_df=valid_df,\n",
    "                                                directory=FLOW_DIR,\n",
    "                                                img_size=IMAGE_SIZE,\n",
    "                                                batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5rIYKaDV8w6"
   },
   "source": [
    "# Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xOdnVa4LvIG1"
   },
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "model = build_model(base_name=BASE_NAME,\n",
    "                    weights=WEIGHTS, \n",
    "                    shape=INPUT_SHAPE,\n",
    "                    pooling=POOLING,\n",
    "                    optimizer=OPTIMIZER,\n",
    "                    name=MODEL_NAME,\n",
    "                    add_top=False,\n",
    "                    metrics=[CohenKappa(num_classes=2, name='kappa')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9pVdiN8ogt7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWm3jVhivnvT",
    "outputId": "fc749b69-28de-4ebd-c080-27576d217800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.2623 - kappa: 0.4587\n",
      "Epoch 1: val_kappa improved from -inf to 0.36642, saving model to /content/drive/MyDrive/MURA/models/ResNet50_tmp_best.h5\n",
      "1134/1134 [==============================] - 781s 683ms/step - loss: 0.2623 - kappa: 0.4587 - val_loss: 0.6242 - val_kappa: 0.3664\n",
      "Epoch 2/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.2298 - kappa: 0.5516\n",
      "Epoch 2: val_kappa improved from 0.36642 to 0.48689, saving model to /content/drive/MyDrive/MURA/models/ResNet50_tmp_best.h5\n",
      "1134/1134 [==============================] - 783s 690ms/step - loss: 0.2298 - kappa: 0.5516 - val_loss: 0.5278 - val_kappa: 0.4869\n",
      "Epoch 3/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.2193 - kappa: 0.5815\n",
      "Epoch 3: val_kappa improved from 0.48689 to 0.57260, saving model to /content/drive/MyDrive/MURA/models/ResNet50_tmp_best.h5\n",
      "1134/1134 [==============================] - 786s 693ms/step - loss: 0.2193 - kappa: 0.5815 - val_loss: 0.4768 - val_kappa: 0.5726\n",
      "Epoch 4/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.2120 - kappa: 0.6014\n",
      "Epoch 4: val_kappa did not improve from 0.57260\n",
      "1134/1134 [==============================] - 784s 691ms/step - loss: 0.2120 - kappa: 0.6014 - val_loss: 0.5106 - val_kappa: 0.4914\n",
      "Epoch 5/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.2059 - kappa: 0.6104\n",
      "Epoch 5: val_kappa did not improve from 0.57260\n",
      "1134/1134 [==============================] - 786s 693ms/step - loss: 0.2059 - kappa: 0.6104 - val_loss: 0.4819 - val_kappa: 0.5628\n",
      "Epoch 6/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.1986 - kappa: 0.6318\n",
      "Epoch 6: val_kappa improved from 0.57260 to 0.58386, saving model to /content/drive/MyDrive/MURA/models/ResNet50_tmp_best.h5\n",
      "1134/1134 [==============================] - 795s 701ms/step - loss: 0.1986 - kappa: 0.6318 - val_loss: 0.4681 - val_kappa: 0.5839\n",
      "Epoch 7/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.1921 - kappa: 0.6443\n",
      "Epoch 7: val_kappa did not improve from 0.58386\n",
      "1134/1134 [==============================] - 785s 692ms/step - loss: 0.1921 - kappa: 0.6443 - val_loss: 0.5612 - val_kappa: 0.5102\n",
      "Epoch 8/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.1860 - kappa: 0.6625\n",
      "Epoch 8: val_kappa did not improve from 0.58386\n",
      "1134/1134 [==============================] - 782s 690ms/step - loss: 0.1860 - kappa: 0.6625 - val_loss: 0.5432 - val_kappa: 0.5356\n",
      "Epoch 9/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.1802 - kappa: 0.6708\n",
      "Epoch 9: val_kappa did not improve from 0.58386\n",
      "1134/1134 [==============================] - 779s 687ms/step - loss: 0.1802 - kappa: 0.6708 - val_loss: 0.6073 - val_kappa: 0.4932\n",
      "Epoch 10/10\n",
      "1134/1134 [==============================] - ETA: 0s - loss: 0.1746 - kappa: 0.6809\n",
      "Epoch 10: val_kappa did not improve from 0.58386\n",
      "1134/1134 [==============================] - 785s 692ms/step - loss: 0.1746 - kappa: 0.6809 - val_loss: 0.6385 - val_kappa: 0.4856\n"
     ]
    }
   ],
   "source": [
    "# Make checkpoint of weights from best epoch\n",
    "best_path = CHECKPOINT_DIR + MODEL_NAME + '_best.h5'\n",
    "checkpoint_best = ModelCheckpoint(filepath=best_path,\n",
    "                                  save_weights_only=True,\n",
    "                                  monitor='val_kappa',\n",
    "                                  mode='max',\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "# Optional - make checkpoint of weights every epoch in case Google Colab runtime disconnects\n",
    "# last_path = CHECKPOINT_DIR + MODEL_NAME + '_last.h5'\n",
    "# checkpoint_last = ModelCheckpoint(filepath=last_path,\n",
    "#                                   save_weights_only=True,\n",
    "#                                   verbose=0)\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_kappa',\n",
    "                           mode='max',\n",
    "                           min_delta=0,\n",
    "                           patience=5,\n",
    "                           verbose=1,\n",
    "                           restore_best_weights=True)\n",
    "\n",
    "hist = model.fit(x=train_flow,\n",
    "                 epochs=EPOCHS,\n",
    "                 validation_data=valid_flow,\n",
    "                 class_weight=get_class_weights(train_df),\n",
    "                 verbose=1,\n",
    "                 callbacks=[checkpoint_best, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfsqNIBuonwb"
   },
   "source": [
    "## Save last epoch weights and training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "GwOBTTHBTECV"
   },
   "outputs": [],
   "source": [
    "# Optional save last weights / full model\n",
    "model.save_weights(CHECKPOINT_DIR + MODEL_NAME + '_last.h5')\n",
    "# model.save(CHECKPOINT_DIR + MODEL_NAME)\n",
    "\n",
    "# Save training history\n",
    "with open(CHECKPOINT_DIR + MODEL_NAME + '_hist', 'wb') as file_pi:\n",
    "        pickle.dump(hist.history, file_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31hgEhzEoeYa"
   },
   "source": [
    "# Evaluation\n",
    "Evaluate trained model on test set, you can also load weights from previous runs to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6w3s13Lbvn-J",
    "outputId": "5266e834-5c44-4e81-ab9a-e60add87adc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 548 validated image filenames belonging to 2 classes.\n",
      "18/18 [==============================] - 4s 197ms/step - loss: 0.4490 - kappa: 0.6176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44896605610847473, 0.6176040172576904]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get test set\n",
    "test_df = get_dataframe(body_part='ALL', split='test', path=DATAFRAME_PATH)\n",
    "\n",
    "# Create dataframe flow for testing set (without augmentation)\n",
    "test_flow = valid_gen.flow_from_dataframe(dataframe=test_df,\n",
    "                                          directory=FLOW_DIR,\n",
    "                                          x_col='filepath',\n",
    "                                          y_col='label',\n",
    "                                          class_mode='binary',\n",
    "                                          target_size=IMAGE_SIZE,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          seed=27,\n",
    "                                          shuffle=False) # Dont shuffle so we can pair images with their filepaths\n",
    "\n",
    "# Optional load weights from previous training\n",
    "# model.load_weights('/content/drive/MyDrive/MURA/models/ResNet50_tmp_best.h5')\n",
    "model.evaluate(x=test_flow, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDqtyexlBLBE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mura.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
